{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import modules"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, enable=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define constants"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "MAX_CAPTION_LENGTH = 30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from Flickr8k"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotation_folder = pathlib.Path(\"data/flickr8k/Flickr8k_text\")\n",
    "image_folder = pathlib.Path(\"data/flickr8k/Flickr8k_Dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a dictionary from all captions\n",
    "captions = (annotation_folder/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "captions = (line.split('\\t') for line in captions)\n",
    "captions = ((image.split('#')[0], f\"<start> {caption} <end>\") for (image, caption) in captions)\n",
    "\n",
    "cap_dict = collections.defaultdict(list)\n",
    "for image, cap in captions:\n",
    "    cap_dict[image].append(cap)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create vocabulary\n",
    "word_counter = collections.Counter([word for caption in captions for word in caption.split()])\n",
    "vocabulary = [word for word, count in word_counter.items() if count >= 5]  # Minimum word count threshold\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_images_and_captions(captions_file):\n",
    "    image_files = (annotation_folder/captions_file).read_text().splitlines()\n",
    "    loaded_images, loaded_captions = [], []\n",
    "    for image_file in image_files:\n",
    "        loaded_images.extend([str(image_folder/image_file)] * len(cap_dict[image_file]))\n",
    "        loaded_captions.extend(cap_dict[image_file])\n",
    "    return loaded_images, loaded_captions"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the image paths and captions\n",
    "train_images, train_captions = load_images_and_captions(\"Flickr_8k.trainImages.txt\")\n",
    "\n",
    "dev_images, dev_captions = load_images_and_captions(\"Flickr_8k.devImages.txt\")\n",
    "\n",
    "test_images, test_captions = load_images_and_captions(\"Flickr_8k.testImages.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(len(cap_dict), len(train_images), len(train_captions), len(dev_images), len(dev_captions), len(test_images), len(test_captions))\n",
    "train_images[:2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "    return image, image_path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create dataset\n",
    "encoded_train = sorted(set(train_images))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(encoded_train)\n",
    "train_dataset = train_dataset.map(load_image).batch(32)\n",
    "\n",
    "encoded_dev = sorted(set(dev_images))\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices(encoded_dev)\n",
    "dev_dataset = dev_dataset.map(load_image).batch(32)\n",
    "\n",
    "encoded_test = sorted(set(test_images))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(encoded_test)\n",
    "test_dataset = test_dataset.map(load_image).batch(32)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ~~Commented~~"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a model to extract the image features\n",
    "inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = inception_v3.input\n",
    "hidden_layer = inception_v3.layers[-1].output\n",
    "\n",
    "model = tf.keras.Model(new_input, hidden_layer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for image, path in tqdm(train_dataset, total=len(train_dataset)):\n",
    "    batch_features = model(image)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_features = p.numpy().decode(\"utf-8\")\n",
    "        print(path_of_features)\n",
    "        break\n",
    "        # np.save(p, bf.numpy())\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the CNN Encoder (Inception V3)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the pre-trained InceptionV3 model without the top layers (for feature extraction)\n",
    "encoder = tf.keras.applications.InceptionV3(include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Freeze the pre-trained model weights (optional, to prevent retraining them)\n",
    "for layer in encoder.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define the RNN Decoder (LSTM)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the embedding dimension for word representation\n",
    "embedding_dim = 256\n",
    "\n",
    "# Define the vocabulary size (number of unique words) based on your preprocessed captions\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "# Create the embedding layer to map words to vectors\n",
    "embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dim, mask_zero=True)\n",
    "\n",
    "# Define the LSTM layers\n",
    "lstm1 = tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "lstm2 = tf.keras.layers.LSTM(256)\n",
    "\n",
    "# Define the dense layer for predicting the next word\n",
    "decoder_dense = tf.keras.layers.Dense(vocabulary_size, activation='softmax')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create model by combining the Encoder and the Decoder"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Input for the image\n",
    "image_input = tf.keras.models.Input(shape=(299, 299, 3))\n",
    "\n",
    "# Extract features from the image using the encoder\n",
    "encoded_features = encoder(image_input)\n",
    "\n",
    "# Define a repeat vector to feed the same features at each step of the decoder\n",
    "decoder_hidden = tf.keras.layers.RepeatVector(MAX_CAPTION_LENGTH)(encoded_features)\n",
    "\n",
    "# Pass the encoded features and hidden state through the LSTM layers\n",
    "decoder_output = lstm1(decoder_hidden)\n",
    "decoder_output = lstm2(decoder_output)\n",
    "\n",
    "# Predict the next word probability distribution\n",
    "decoder_logits = decoder_dense(decoder_output)\n",
    "\n",
    "# Define the model by connecting inputs and outputs\n",
    "model = tf.keras.models.Model(inputs=image_input, outputs=decoder_logits)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model on the prepared training data for multiple epochs\n",
    "model.fit(train_images, train_captions, epochs=10, validation_data=(dev_images, dev_captions))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.save('image_captioning_model.h5')"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the saved model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = tf.keras.models.load_model('image_captioning_model.h5')"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate captions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_caption(image_path):\n",
    "  # Load and preprocess the image\n",
    "  image, _ = load_image(image_path)\n",
    "  \n",
    "  # Get encoded features from the image\n",
    "  encoded_image = encoder.predict(np.expand_dims(image, axis=0))\n",
    "  \n",
    "  # Initialize variables for caption generation\n",
    "  max_len = MAX_CAPTION_LENGTH  # Define the maximum caption length\n",
    "  sequence = [word_to_index['<start>']]\n",
    "  \n",
    "  # Generate caption word by word\n",
    "  for _ in range(max_len):\n",
    "    # One-hot encode the current sequence\n",
    "    current_sequence = np.array([sequence])\n",
    "    predicted_probs = model.predict(current_sequence)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    # Check for end of caption or maximum length reached\n",
    "    if predicted_index == word_to_index['<end>'] or len(sequence) >= max_len:\n",
    "      break\n",
    "    \n",
    "    sequence.append(predicted_index)\n",
    "  \n",
    "  # Convert the predicted word indices to actual words\n",
    "  caption = [index_to_word[idx] for idx in sequence[1:-1]]  # Exclude start and end tokens\n",
    "  return ' '.join(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Real Image"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
